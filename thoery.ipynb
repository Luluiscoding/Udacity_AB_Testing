{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing Theory Notes  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTLINE**:  \n",
    "________________\n",
    "- **Preparation**\n",
    "  - Import Packages\n",
    "  - Define Functions\n",
    "  - Some statistical knowledge\n",
    "- **What is A/B Testing?**\n",
    "- **Choosing and Characterizing**\n",
    "  - Experiment\n",
    "  - Set Our Business Objective\n",
    "  - Refining the Customer Funnel\n",
    "  - Metrix Choice\n",
    "    - About Data\n",
    "    - How To Validate Metrics\n",
    "    - Make Sure The Definitions Are Clear\n",
    "    - Filtering and Segmenting\n",
    "    - Build Intuition\n",
    "    - Characterize\n",
    "  - Hypothesis\n",
    "- **Designing an Experiment**\n",
    "  - Unit of Diversion\n",
    "  - Target Population\n",
    "  - Sizing\n",
    "  - Duration\n",
    "  - Learning Effect\n",
    "- **Analyzing Results**  \n",
    "  - Sanity Check\n",
    "    - Choosing Invariants\n",
    "  - Handle Evaluation with Single or Multiple Metrics\n",
    "    - Single Metric\n",
    "    - Multiple Metrics\n",
    "- **Conclusion**\n",
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation  \n",
    "\n",
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Strategy: \n",
    "## (1) For a bunch of Ns, compute the z_star by achieving desired alpha, then\n",
    "## (2) compute what beta would be for that N using the acquired z_star. \n",
    "## (3) Pick the smallest N at which beta crosses the desired value\n",
    "\n",
    "# Inputs: \n",
    "#         The desired alpha for a two-tailed test\n",
    "# Returns: \n",
    "#         The z-critical value\n",
    "def get_z_star(alpha):\n",
    "    return -norm.ppf(alpha / 2)\n",
    "# Inputs:\n",
    "#        z-star: The z-critical value\n",
    "#        s: The standard error of the metric at N=1\n",
    "#        d_min: The practical significance level\n",
    "#        N: The sample size of each group of the experiment\n",
    "# Returns: \n",
    "#        The beta value of the two-tailed test\n",
    "def get_beta(z_star, s, d_min, N): \n",
    "    SE = s / math.sqrt(N)\n",
    "    return norm.cdf(z_star * SE, loc=d_min, scale=SE)\n",
    "\n",
    "# Inputs:\n",
    "#        s: The standard error of the metric with N=1 in each group\n",
    "#        d_min: The practical significance level\n",
    "#        Ns: The sample sizes to try\n",
    "#        alpha: The desired alpha level of the test\n",
    "#        beta: The desired beta level of the test\n",
    "# Returns: \n",
    "#        The smallest N out of the given Ns that will achieve the desired\n",
    "#        beta. There should be at least N samples in each group of the experiment.\n",
    "#        If none of the given Ns will work, returns -1. N is the number of samples in each group.\n",
    "def required_size(s, d_min, Ns=range(1, 20001), alpha=0.05, beta=0.2):\n",
    "    for N in Ns:\n",
    "        if get_beta(get_z_star(alpha), s, d_min, N) <= beta:\n",
    "            return N\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a plot to show decision boundary\n",
    "def decision(lowerCI, upperCI, x):\n",
    "    y = 2.5\n",
    "    min_x = min(lowerCI, upperCI, x)  # Minimum x-value\n",
    "    max_x = max(lowerCI, upperCI, x)\n",
    "    extend = (max_x - min_x) * 0.5  # Extend the x-axis by 50% of the range\n",
    "    dot_positions = [lowerCI, upperCI, x]\n",
    "    fig, ax = plt.subplots(figsize=(3, 0.5))\n",
    "    ax.axis('off')\n",
    "    ax.hlines(y, xmin=min_x-extend, xmax=max_x+extend, linestyle='-')\n",
    "    for position in dot_positions:\n",
    "        if position == lowerCI or position == upperCI:\n",
    "            ax.vlines(position, ymin=y-0.05, ymax=y+0.05, color='black', linewidth=1)\n",
    "        else:\n",
    "            ax.scatter(position, y, color='red', s=100, zorder=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print table in markdown format\n",
    "def mkdtable(df):\n",
    "    markdown_table = df.to_markdown(index=False)\n",
    "    # Print the Markdown code\n",
    "    print(markdown_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some statistical knowledge\n",
    "\n",
    "#### **What is $\\alpha$ and $\\beta$**\n",
    "Type I error: $\\alpha=P(\\{\\text{reject null} | \\text{null true} \\})$  \n",
    "Type II error: $\\beta=P(\\{\\text{fail to reject null} | \\text{null false} \\})$  \n",
    "Power: $1-\\beta$  \n",
    "\n",
    "<img src=\"image/20.png\" width=\"700\"> \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is A/B testing?  \n",
    "\n",
    "A/B testing is a general methodology used online when you want to test out a new product or a feature.  \n",
    "  \n",
    "What you're doing is you're going to take two separate users and you show one set (a control set), your existing product or feature and then another set (your experiment), the new version and what you're going to do is we're going to say how do these users respond differently and which is to determine which version of this feature is better.  \n",
    "  \n",
    "AB testing is really useful for helping you climb to the peak of your current mountain. But if you want to figure out whether you want to be on this mountain or another mountain A/B testing isn't so useful.  \n",
    "  \n",
    "Some categories companies practiced:   \n",
    "- New Feature Additions [Amazong Personalized recommendations]()  \n",
    "- ranking changes [Linkin ranking changes]()  \n",
    "- Some changes that you are not sure a user would notice. [Amazon page load time]()  [Google web search time]()  \n",
    "  \n",
    "But There are some things you cannot do with AB testing:  \n",
    "- A/B testing isn't a useful testing out new experiences.  \n",
    "- Some problems need long time to test, like the rental.  \n",
    "\n",
    "The key thing to remember is that the goal of doing A/B tests is to determine whether or not this new product or this new feature is something that users will like.  The goal in A/B tests is to design an experiment that is going to be robust and give you repeatable results. so you can actually make a good decision about whether or not to actually launch that product or feature.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing and Characterizing Metrics\n",
    "\n",
    "### Experiment\n",
    "\n",
    "> change the **start now** button from orange to pink\n",
    "\n",
    "### Set our Business objective  \n",
    "\n",
    "> - helping students get jobs  \n",
    "> - financial sustainability\n",
    "\n",
    "### Refining the customer funnel\n",
    "\n",
    "> <img src=\"image/1.png\" width=\"300\">   \n",
    "> \n",
    "> Here are related actions in each step:   \n",
    "> \n",
    "> <img src=\"image/2.png\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By refining the funnel and actions, we are able to draw the metrics in each step and then select the appropriate ones.  \n",
    "\n",
    "> In each stage, we collect the following data:  \n",
    "> - count: the number of users who reach that point  \n",
    ">   \n",
    "> - rate or probabilities: $\\frac{\\text{number of users of current point}}{\\text{number of users of previous point}}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Choices \n",
    "- Invariant Metrics:  \n",
    "  - Invariant metrics are performance metrics that are expected to <span style=\"background-color: #edd2c4\">remain consistent</span> between the control group (the group that receives the current or existing experience) and the experimental group (the group that receives the new or modified experience). These metrics are used to ensure that the groups are comparable before drawing conclusions about the impact of the experimental change. Invariant metrics are also known as \"pre-treatment\" or \"sanity-check\" metrics.  \n",
    "  - The purpose of invariant metrics is to validate that the <span style=\"background-color: #edd2c4\">randomization</span> process and the assignment of users to the control and experimental groups are working correctly. If there are significant differences in these metrics between the two groups before the experiment, it can indicate issues with randomization or that the groups are not comparable, which would undermine the validity of the test results.  \n",
    "- Evaluation Metrics:  \n",
    "  - Evaluation metrics are the <span style=\"background-color: #edd2c4\">key performance indicators (KPIs)</span> or measurements used to assess the impact and effectiveness of changes made in an experiment. These metrics are used to determine whether the variant (the group receiving the change) outperforms the control (the group with the existing experience) in a statistically significant and meaningful way.  \n",
    "  - Evaluation metrics serve as the primary means of quantifying and comparing the outcomes of the two groups to make data-driven decisions about whether to implement the change or not. The choice of evaluation metrics depends on the specific goals and objectives of the A/B test. Different experiments may have different evaluation metrics that align with their intended outcomes.  \n",
    "\n",
    "> Some metrics to be considered:  \n",
    "> 1. Click-Through-Rate (CTR) on \"start now\" button\n",
    "> 2. Click-through-probability (CTP) on \"start now\" button\n",
    "> 3. Probability of progressing from course list to course page\n",
    "> 4. Probability of progressing from course page to enrolling\n",
    "> 5. Probability that enrolled student pays for coaching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **About data:**   \n",
    "   \n",
    "Sometimes there are some difficult metrics, which means we do not have access to data or it takes a long time. Here are some techniques to get data:  \n",
    "- External Data ([Additional Techniques for Brainstorming and Validating Metrics](extension://bfdogplmndidlpjfhoijckpakkdjkkil/pdf/viewer.html?file=https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fgae-supplemental-media%2Fadditional-techniquespdf%2Fadditional_techniques.pdf))\n",
    "  - Companies collect fairly granular data (like market share)\n",
    "  - Companies run surveys of users\n",
    "  - Academic research  \n",
    "- Own Data\n",
    "  - Using existing data(Retrospective/observational analysis):  \n",
    "     - look at the metrics you are interested in or just measurements you take from your site, change in response to changes you made in the past. That is good to get a baseline, and it can also help you develop theories. These studies show you correlations, not causation.\n",
    "  - Gather new data: (<span style=\"color: green\">**+**</span> stands for positive part, <span style=\"color: red\">**-**</span> stands for negative part)\n",
    "     - User experience research (UER):  \n",
    "       <span style=\"color: green\">**+**</span> good for brainstorming  \n",
    "       <span style=\"color: green\">**+**</span> can use special equipment  \n",
    "       <span style=\"color: red\">**-**</span>  want to validate results          \n",
    "     - Focus Group:  \n",
    "       <span style=\"color: green\">**+**</span> get feedback on hypotheticals  \n",
    "       <span style=\"color: red\">**-**</span>  run the risk of group think  \n",
    "     - Surveys:  \n",
    "       <span style=\"color: green\">**+**</span> useful for metrics you cannot directly measure  \n",
    "       <span style=\"color: red\">**-**</span>  cannot directly compare to other results.    \n",
    "<img src=\"image/3.png\" width=\"300\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **How to validate metrics**:  \n",
    "1. Running experiment:   \n",
    "   measure if the metric is actually going to move as we make changes. (sensitivity and robustness)\n",
    "2. Talk to your colleagues  \n",
    "\n",
    "#### **Make sure the definitions are clear**:  \n",
    "  \n",
    "Step1: Fully define what data we are going to look at to actually compute the metric.  \n",
    "Step2: Given those event, how do I summarize my metric? (build intuition)  \n",
    "  \n",
    "Example:  \n",
    "<img src=\"image/4.png\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Filtering and Segmenting**:  \n",
    "  \n",
    "Increase the <span style=\"background-color: #edd2c4\">sensitivity and robust</span> about the experiment.  \n",
    "The goal is <span style=\"background-color: #edd2c4\">de-bias</span> the data  \n",
    "Good for evaluating definition and build intuition  \n",
    "  \n",
    "Why do we need to filter and segment the data:   \n",
    "- External reasons: malicious mess up the metrics  \n",
    "- Internal reasons: what happens if your change only impact a subset of your traffic. So, just filter the affected traffic, do not dilute the data, then you can actually increase the power and sensitivity of your experiment.  \n",
    "  \n",
    "Example:  \n",
    "<img src=\"image/5.png\" width=\"800\">   \n",
    "<img src=\"image/6.png\" width=\"800\">    \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Build Intuition**  \n",
    "  \n",
    "Summarize all individual events into a single summary metric (numbers $\\rightarrow$ median, mean, mode or etc.)   \n",
    "  \n",
    "1. <span style=\"background-color: #90ccc3\">Sensitivity and Robustness</span>\n",
    "- To compute or control sensitivity:\n",
    "   - Running experiment or using the experiment we have:  \n",
    "       see if the metrics we were interested in is actually respond to the changes in a simple experiment. We should tell if they are actually moving in a way that intuitively makes sense. We can also use what were called A vs. A experiment, that is, the experiment where you do not change anything, just compare people who saw the same thing to each other.\n",
    "   - Retrospective analysis\n",
    "       Measuring sensitivity and robustness:  \n",
    "       <img src=\"image/7.png\" width=\"800\">  \n",
    "       <img src=\"image/8.png\" width=\"800\">    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. <span style=\"background-color: #90ccc3\">Distribution</span>  \n",
    "- Doing the retrospective analysis and compute the histogram  \n",
    "- Some common distribution:  \n",
    "   - [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution):  \n",
    "\tApplication: measure the average stay time on the results page before traveling to a result. \n",
    "   - [Zip-fan or Pareto distribution](https://en.wikipedia.org/wiki/Pareto_distribution):  \n",
    "\tthe probability of a more extreme value, $z$, decreases like $\\frac{1}{z}$ (or $\\frac{1}{z^{e}}$).  \n",
    "   This distribution also comes up in other rare events such as the frequency of words in a text (the most common word is really really common compared to the next word on the list). \n",
    "   These types of heavy-tailed distributions are common in internet data.\n",
    "\t\n",
    "\tyou may have data that is a composition of different distributions. This forms what is called a mixture distribution that can be hard to detect or characterize well.  \n",
    "   \n",
    "- Four categorize to memorize:  \n",
    "   - Sums and counts  \n",
    "   - mean, median, 25%,50%,75% percentile  \n",
    "   - probability and rate  \n",
    "   - ratios  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Characterize**:  \n",
    "\n",
    "Variability is used to sizing the experiment and analyze the confidence interval.  \n",
    "  \n",
    "<span style=\"color: grey\">Sizing will be introduced later, we firstly learn how to calculate a confidence interval.</span>  \n",
    "  \n",
    "In order to calculate a confidence interval, we need:  \n",
    "- Variance  \n",
    "- Distribution  \n",
    "   - example: binomial distribution  \n",
    "      The PMF of getting exactly k successes in n independent Bernoulli trials is:  \n",
    "        \n",
    "      $f(k,n,p)=Pr(k,n,p)=Pr(X=K)= \\binom{n}{k} \\cdot p^k \\cdot (1-p)^{n-k}$  \n",
    "        \n",
    "      $ \\text{standard error(SE)} = \\sqrt{\\frac{p \\cdot (1 - p)}{n}}$  \n",
    "        \n",
    "      As N get larger, Binomial distribution is close to a Normal distribution.  \n",
    "        \n",
    "      $ \\text{m (mean)} = z * SE$, where $z=\\frac{x-\\mu}{\\sigma}$\n",
    "\n",
    "Calculating Variability:  \n",
    "  \n",
    "<img src=\"image/9.png\" width=\"600\"> \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-parametric answers: you have a way to analyze the data without making the assumption of the distribution: sign test.  \n",
    "  \n",
    "Sometimes you do not want to launch unless you meet some threshold (your practical significance level). Firstly look at the summary statistic metric:  \n",
    "- <span style=\"background-color: #90ccc3\">If it is nice and normal, using normal distributed CI and variant.</span>  \n",
    "    <img src=\"image/11.png\" width=\"800\">   \n",
    "- <span style=\"background-color: #90ccc3\">If it is funnier or you really need robust, then calculate the non-parametric CI (directly estimate CI).</span>  \n",
    "   - STEP 1: take all your differences and put them in order.  \n",
    "      <img src=\"image/10-1.png\" width=\"500\">   \n",
    "   - STEP 2:  if you want a 95% confidence interval, select a box that includes only 95% of the values. (Eg. We have 20 data point, which means dropping the highest and the lowest gives us a 90% confidence level. Then, empirical standard deviation=0.059*1.65=0.097)\n",
    "      <img src=\"image/10-2.png\" width=\"500\"> \n",
    "   - **Reasons to use the empirical method:**  \n",
    "\n",
    "      When you compute the variance of the metric, you are making an assumption about the underlying distribution of the data. Even for some simple metrics, the analytical estimate of the variance ended up being an underestimate. Using <span style=\"color: red\">A versus A experiments</span> again.  \n",
    "  \n",
    "      - <span style=\"background-color: #90ccc3\">A/A experiment:</span>   \n",
    "        a control A and another control A. If you see a lot of variability in a metric in an A versus A test, it is probably too sensitive to be useful in. We can kind of pin down the variability with these A/A test. In order to get a good sense, sometimes we need to run thousand times. There is a diminishing return as you run more A versus A tests. The key rule of thumb to keep in mind is that the standard deviation is going to be proportional to the square root of the number of sample. Or, we can run one big A/A experiment.   \n",
    "  \n",
    "      - <span style=\"background-color: #90ccc3\">Bootstrap:</span>  \n",
    "        <img src=\"image/12.png\" width=\"500\">  \n",
    "\n",
    "        The idea in bootstrapping is that you Increase the <span style=\"background-color: #edd2c4\">run one experiment</span>. <span style=\"color: #e0e0e0\">And even though in our spreadsheet we only showed one number from each group, for each experiment, the click-through probability, those numbers were actually calculated from a lot of individual data points, about a bunch of individual page views and clicks.</span> Then you Increase the <span style=\"background-color: #edd2c4\">take a random sample</span> of those data points Increase the <span style=\"background-color: #edd2c4\">from each side</span> of the experiment, and Increase the <span style=\"background-color: #edd2c4\">calculate</span> the CTP based on that random sample as if it were a full experimental group. Then you Increase the <span style=\"background-color: #edd2c4\">record the difference</span> in click through probabilities and use that as a simulated experiment. Then you Increase the <span style=\"background-color: #edd2c4\">repeat</span> this process over and over, Increase the <span style=\"background-color: #edd2c4\">recording</span> the results. And you can use the results as if you had run multiple experiments, even though you really only ran one big experiment.  \n",
    "      \n",
    "      The advantage of running lots of different <span style=\"background-color: #edd2c4\">A/A tests</span> is because if your experiment system is itself complicated, it is actually a very good test of your system <span style=\"color: #e0e0e0\">(eg. Is your randomization function truly random? Bias? Population effects?).</span> In reality, what we have is a whole gradation of different methods. If you are starting out and you are running your first experiment using a relatively simple metric, do the analytical estimate of variance.  \n",
    "        \n",
    "      If you are starting to push towards more complicated metrics or you are running more and more features through, at that point, you might want to consider at least doing the <span style=\"background-color: #edd2c4\">bootstrap</span>. Now if your bootstrap estimate is agreeing with your analytical estimate, you can probably move on and you do not have to worry about it. If it isn't agreeing with your analytical estimate, at that piont you may want to consider runniung a lot of A/A tests and really digging into understanding what's going on.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Total number of courses complete (take too much time)  \n",
    ">   \n",
    "> - CTR $= \\frac{\\text{Number of clicks}}{\\text{number of page views}}$  \n",
    ">     \n",
    "> - CTP $= \\frac{\\text{unique visitors who click}}{\\text{unique visitors to page}}$  \n",
    "> \n",
    "> In this case, we choose CTP.  \n",
    "\n",
    "### Hypothesis\n",
    "> Change color will increase the CTP of the button"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing an experiment  \n",
    "\n",
    "### Choose Subject (unit of diversion)  \n",
    "\n",
    " <img src=\"image/13.png\" width=\"1000\">  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to choose between different diversions?**  \n",
    "\n",
    "- Consistency  \n",
    "- Ethical Consideration\n",
    "- Variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Population  \n",
    "\n",
    "If you thin you can identify what population will be affected by your experiment, you might want to target your experiment to that traffic. How does this affect the variability of the traffic?  \n",
    "Changing the unit of diverging can change the empirical estimate of variability. Filtring your traffic can change it too. \n",
    "\n",
    "> EXAMPLE:  \n",
    "> - You want to control the change that will only affect users in New Zealand to see whether it increases CTP.  \n",
    "> \n",
    "> After running the experiment, there are($X$ stands for people who clicked):  \n",
    "> \n",
    "> - $N_{cont}=6021, X_{cont}=302$  \n",
    "> - $N_{exp}=5979, X_{exp}=374$   \n",
    "> \n",
    "> Based on this, we can calculate:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Zealand Control p_estimate(baseline probability)=5.02%\n",
      "New Zealand Experiment p_estimate=6.26%\n",
      "New Zealand Pool p_estimate=5.63%\n",
      "New Zealand Standard Erro=0.0042\n"
     ]
    }
   ],
   "source": [
    "# New Zealand data\n",
    "N_nz_cont=6021\n",
    "N_nz_exp=5979\n",
    "X_nz_cont=302\n",
    "X_nz_exp=374\n",
    "p_nz_estimate_cont= X_nz_cont/N_nz_cont\n",
    "print(f'New Zealand Control p_estimate(baseline probability)={p_nz_estimate_cont*100:.2f}%')\n",
    "p_nz_estimate_exp= X_nz_exp/N_nz_exp\n",
    "print(f'New Zealand Experiment p_estimate={p_nz_estimate_exp*100:.2f}%')\n",
    "p_nz_estimate_pool=(X_nz_cont+X_nz_exp)/(N_nz_cont+N_nz_exp)\n",
    "print(f'New Zealand Pool p_estimate={p_nz_estimate_pool*100:.2f}%')\n",
    "SE_nz_pool=np.sqrt(p_nz_estimate_pool*(1-p_nz_estimate_pool)*(1/N_nz_cont+1/N_nz_exp))\n",
    "print(f'New Zealand Standard Erro={SE_nz_pool:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We also have other countries' data:  \n",
    "> - $N_{cont}=50000, X_{cont}=2500$  \n",
    "> - $N_{exp}=50000, X_{exp}=2500$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Countries' data\n",
    "N_other_cont=50000\n",
    "N_other_exp=50000\n",
    "X_other_cont=2500\n",
    "X_other_exp=2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Control p_estimate(baseline probability)=5.00%\n",
      "Global Experiment p_estimate=5.13%\n",
      "Global Pool p_estimate=5.07%\n",
      "Global Pool Standard Error=0.0013\n"
     ]
    }
   ],
   "source": [
    "# Global data:\n",
    "N_g_cont=N_nz_cont+N_other_cont\n",
    "N_g_exp=N_nz_exp+N_other_exp\n",
    "X_g_cont=X_nz_cont+X_other_cont\n",
    "X_g_exp=X_nz_exp+X_other_exp\n",
    "p_g_estimate_cont= X_g_cont/N_g_cont\n",
    "print(f'Global Control p_estimate(baseline probability)={p_g_estimate_cont*100:.2f}%')\n",
    "p_g_estimate_exp= X_g_exp/N_g_exp\n",
    "print(f'Global Experiment p_estimate={p_g_estimate_exp*100:.2f}%')\n",
    "p_g_estimate_pool=(X_g_cont+X_g_exp)/(N_g_cont+N_g_exp)\n",
    "print(f'Global Pool p_estimate={p_g_estimate_pool*100:.2f}%')\n",
    "SE_g_pool=np.sqrt(p_g_estimate_pool*(1-p_g_estimate_pool)*(1/N_g_cont+1/N_g_exp))\n",
    "print(f'Global Pool Standard Error={SE_g_pool:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||New Zealand|Global|\n",
    "|----|----|----|\n",
    "|$\\hat p_{cont}$|5.02%|5.00%|\n",
    "|$\\hat p_{exp}$|6.26%|5.13%|\n",
    "|$\\hat p_{pool}$|5.63%|5.07%|\n",
    "|$ SE_{pool}$|0.0042|0.0013|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that the variability of Global is much lower than New Zealand. Because there are more data globally.  \n",
    "In practice, the data will actually be a mix of different populations almost every time. When you filter, you're going to get a smaller but more uniform population, which means that for the same number of data points, the variability of the filtered data is likely to be lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in p_estimate=0.0013\n",
      "Margin of Error=0.0026\n",
      "Confidence Interval=-0.0012 to 0.0039\n"
     ]
    }
   ],
   "source": [
    "# Global significance test\n",
    "diff_estimate = p_g_estimate_exp-p_g_estimate_cont\n",
    "print(f'Difference in p_estimate={diff_estimate:.4f}')\n",
    "m=SE_g_pool*1.96\n",
    "print(f'Margin of Error={m:.4f}')\n",
    "print(f'Confidence Interval={diff_estimate-m:.4f} to {diff_estimate+m:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 0 is within the confidence interval, or to say, the difference is neither larger than the margin of error nor samller than the negative margin of error, we cannot reject the null hypothesis, so it is not significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in p_estimate=0.0124\n",
      "Margin of Error=0.0083\n",
      "Confidence Interval=0.0041 to 0.0206\n"
     ]
    }
   ],
   "source": [
    "# New Zealand significance test\n",
    "diff_estimate = p_nz_estimate_exp-p_nz_estimate_cont\n",
    "print(f'Difference in p_estimate={diff_estimate:.4f}')\n",
    "m=SE_nz_pool*1.96\n",
    "print(f'Margin of Error={m:.4f}')\n",
    "print(f'Confidence Interval={diff_estimate-m:.4f} to {diff_estimate+m:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 0 is not within the confidence interval, or to say, the difference is larger than the margin of error, we can reject the null hypothesis, so it is significant.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Population VS. Cohort**  \n",
    "  \n",
    "In the population, you have a whole group of users. But within that population you can define what's called a cohort. And typically, this means people who enter the experiment at the same time.  \n",
    "  \n",
    "Cohort usually means that you define an entering class and you only look at users who entered your experiment on both sides around the same time, and you go forward from there.  \n",
    "  \n",
    "Cohorts are harder to analyze, and they're going to take more data because you'll lose users. So typically, you only want to use them when you're looking for user stability.  \n",
    "  \n",
    "When to use a cohort instead of a population:  \n",
    "- looking for learning effects  \n",
    "- examing user retention  \n",
    "- want to increase user activity   \n",
    "- anything requiring user to be established  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizing  \n",
    "\n",
    "We need a fair amount of user data to make the experienment work.  \n",
    "\n",
    "**How variability affects sizing?**\n",
    "\n",
    "> Example:   \n",
    "> - Audacity includes promotions for coaching next to videos. Experiment: change wording of message.  \n",
    "> \n",
    "> Metric: \n",
    "> - $CTR=\\frac{\\# clicks}{\\# pageviews}$  \n",
    "> \n",
    "> Unit of diversion: \n",
    "> - pageview or cookie  \n",
    ">\n",
    "> Here are same data:\n",
    "> - empirical estimate with 5000 pageviews  \n",
    "> - By sampling pageview, we get SD=0.00515  \n",
    "> - By sampling cookie, we get SD=0.0119  \n",
    "> - To calculate size, we assume SE ~ $\\frac{1}{\\sqrt{N}}$, that is:  \n",
    "> <img src=\"image/14.png\" width=\"350\">   \n",
    "> - In this case, we set practical significance $d_{min}=0.02$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3532"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = math.sqrt(0.1 * 0.9 * 2)\n",
    "required_size(s, d_min=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2603\n",
      "13894\n"
     ]
    }
   ],
   "source": [
    "SD_pageview=0.00515\n",
    "SD_cookie=0.0119\n",
    "d_min=0.02\n",
    "# cookie-based\n",
    "print(required_size(s=SD_pageview*math.sqrt(5000), d_min=d_min))\n",
    "# browser-based\n",
    "print(required_size(s=SD_cookie*math.sqrt(5000), d_min=d_min))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to  translate our ideal size into a set of practical decisions. We need to know:  \n",
    "- what's the duration of the experiment that I want to run?  \n",
    "   <span style=\"color: grey\">let's say we're and we need **1 million** cookies in our experiment and our control combined. Now, if you only get a **100,000** cookies visiting your site on any given day. That means that if you want to run **50%** of your traffic through the experiment and 50% through the control, you need to run your experiment control for **10 days**. Now, another choice is to run your experiment at **25%** each, say, it's because you want to run another test, then you'd have to run your experiment for **20 days** as opposed to 10. And that's how, the duration of your experiment, is related to the proportion of traffic that you're sending through your experiment.</span>  \n",
    "   <span style=\"color: grey\"></span>\n",
    "- when do I want to run the experiment?\n",
    "- Is it going to overlap something that's important?\n",
    "- what fraction of your traffic you're going to send through the experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Effect\n",
    "\n",
    "when a user first sees a change, they're going to tend to react in one of these two ways. But over time they're going to probably plateau to a very different behavior. Now, the key issue with trying to measure a learning effect is time. It takes time for you just to actually adapt to a change and often times you don't have the luxury of taking that much time to make a decision.   \n",
    "  \n",
    "- choosing the unit of diversion correctly\n",
    "- dosage\n",
    "- risk and duration\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analzing results\n",
    "\n",
    "We are going to see what you can or cannot conclude fro the data that you've captured.  \n",
    "Remember that running A/B testing is an iterative process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks\n",
    "\n",
    "Population sizing metrics:   \n",
    "- based on your unit of diversion.What you're really checking there is that your experiment population and your control populations are actually comparable.  \n",
    "  \n",
    "Actual invariants:  \n",
    "- those metrics that shouldn't change when you run your experiment. And what you want to do is you want to test that they didn't change.  \n",
    "\n",
    "####  **Choosing Invariants:**  \n",
    "  \n",
    "<img src=\"image/15.png\" width=\"700\">   \n",
    "<img src=\"image/16.png\" width=\"700\">   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **checking invariants**\n",
    "\n",
    "> - Running experiments for 2 weeks\n",
    "> - Unit of diversion: Cookies  \n",
    ">   \n",
    "> Here is the data we collect:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Day   |   # cookies control |   # cookies experiment |\n",
      "|:------|--------------------:|-----------------------:|\n",
      "| Mon   |                5077 |                   4877 |\n",
      "| Tues  |                5495 |                   4729 |\n",
      "| Wed   |                5294 |                   5063 |\n",
      "| Thurs |                5446 |                   5035 |\n",
      "| Fri   |                5126 |                   5010 |\n",
      "| Sat   |                3382 |                   3193 |\n",
      "| Sun   |                2891 |                   3226 |\n",
      "| Day   |   # cookies control |   # cookies experiment |\n",
      "|:------|--------------------:|-----------------------:|\n",
      "| Mon   |                5029 |                   5092 |\n",
      "| Tues  |                5166 |                   5048 |\n",
      "| Wed   |                4902 |                   4985 |\n",
      "| Thurs |                4923 |                   4805 |\n",
      "| Fri   |                4816 |                   4741 |\n",
      "| Sat   |                3411 |                   2939 |\n",
      "| Sun   |                3496 |                   3075 |\n"
     ]
    }
   ],
   "source": [
    "# print table in markdown format\n",
    "\n",
    "data_week1 = {'Day': [\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"],\n",
    "              '# cookies control': [5077,5495,5294,5446,5126,3382,2891],\n",
    "              '# cookies experiment': [4877,4729,5063,5035,5010,3193,3226]}\n",
    "df_w1 = pd.DataFrame(data_week1)\n",
    "mkdtable(df=df_w1)\n",
    "\n",
    "data_week2 = {'Day': [\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"],\n",
    "                '# cookies control': [5029,5166,4902,4923,4816,3411,3496],\n",
    "                '# cookies experiment': [5092,5048,4985,4805,4741,2939,3075]}\n",
    "df_w2 = pd.DataFrame(data_week2)\n",
    "mkdtable(df=df_w2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"background-color: #90ccc3\">See the total numbers of each group to check the overall difference.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total control number=64454\n",
      "Total experiment number=61818\n",
      "Total number=126272\n"
     ]
    }
   ],
   "source": [
    "N_cont=df_w1['# cookies control'].sum()+df_w2['# cookies control'].sum()\n",
    "N_exp=df_w1['# cookies experiment'].sum()+df_w2['# cookies experiment'].sum()\n",
    "print(f'Total control number={N_cont}')\n",
    "print(f'Total experiment number={N_exp}')\n",
    "N_total=N_cont+N_exp\n",
    "print(f'Total number={N_total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; We can see that there are more data in the control group, now the question is that:  \n",
    "- <span style=\"background-color: #90ccc3\">How would you figure out whether this difference is within expectations?</span>  \n",
    "    \n",
    "  This is similar to the event: flipping a fair coin, I want to know if it's surprising that the coin came up heads 64,454 times. The total sample size here is about 120,000, which is definitely enough to assume a normal distribution.  \n",
    "    \n",
    "  - 1. Compute standard deviation of binomial with probability 0.5 of success.  \n",
    "  - 2. Multiply by [z-score](https://www.z-table.com/) to get margin of error.  \n",
    "  - 3. Compute confidence interval around 0.5.  \n",
    "  - 4. Check whether observed fraction is within interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation=0.0014\n",
      "Margin of Error=0.0028\n",
      "Confidence Interval=0.4972 to 0.5028\n",
      "Observed fraction=0.5104\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAA6CAYAAACUGjTOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADHElEQVR4nO3dv08TYRzH8c81+KMVDEJQGlkoYXHTRWOcYWDEGUbjAAmLo8DmCCzOMAsbJOwm6iL/gGkcwCMRSA2kRSA9h0eaQKI9oNfrw/f9Wppc75pvE965p3dAgyiKIgEwIZP2AACah+ABQwgeMITgAUMIHjCE4AFDCB4whOABQwgeMITgAUMIvo4wDDUzM6MwDNMeJTYfZ0ZzEHwdYRhqdnbWq3h8nBnNQfCAIQQPGELwgCEEDxhC8IAhBA8YQvCAIQQPGNKW9gDAlUWRtLsrHRxI7e1Sd7cUBGlP1ZI4w8NfpZI0Py8NDko9PVJ/v3scHHTbS6W0J2w5BA8/ra9LfX3S1JRULJ59rlh02/v63H6oIXj4Z31dGhmRKhW3nD//1Qqn2yoVtx/R1xA8/FIqSaOjLuhq9f/7Vqtuv9FRlvd/ETz8srgolcv1Yz9Vrbr9l5aSncsTiVylLx+dJPGyqagcn9QefXlfPs4cSxTp9sKCAkkXuQYfSYrm53X46rV3V+9zNxubaCLBP3p7fT4z/d7+Jkl6+f6TbvX+THmaeHycOY575V/aOH+BLoYgihQUi3r+5oNK2bsJTJac7+9GGvp6LOnhjTvHh1c6vv2o0qBJ/BUk8e2x12kZubHxVS+ePdXHz1/0+PGTtMeJxceZY9nZUe5h/tKHl39su1/K8YgXS/pGD5mm7I222qMv78vHmWPJP5AGBtx99oucp4JAKhSU673v3Wf4RmNJD38EgTQxcbljJyfNxy4RPHwzPi7lclIm5o9uJuP2HxtLdi5PEDz80tkpLS+7s3W96DMZt9/KijsOBA8PDQ9Lq6tSNuuCPr9UP92WzUpra9LQUDpztiCCh5+Gh6XNTWluTioUzj5XKLjtW1vEfs41uoQLczo73cW4iQlpb0/a35c6OqSuLi7Q/QPBw39B4O6ve3aPPQ0s6QFDCB4whOABQwgeMITgAUMIHjCE4AFDCL6OfD6v6elp5fOX/zvsZvNxZjRHIv8AA0Br4gwPGELwgCEEDxhC8IAhBA8YQvCAIQQPGELwgCEEDxhC8IAhfwDJhtPXi6XZogAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 300x50 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute standard deviation of binomial with probability 0.5 of success.  \n",
    "p=0.5\n",
    "SD=np.sqrt(p*(1-p)/N_total)\n",
    "print(f'Standard Deviation={SD:.4f}')\n",
    "\n",
    "# Multiply by z-score to get margin of error.  \n",
    "m=SD*1.96\n",
    "print(f'Margin of Error={m:.4f}')\n",
    "\n",
    "# Compute confidence interval around 0.5.  \n",
    "lc=0.5-m\n",
    "uc=0.5+m\n",
    "print(f'Confidence Interval={lc:.4f} to {uc:.4f}')\n",
    "\n",
    "# Check whether observed fraction is within interval.\n",
    "p_hat=N_cont/N_total\n",
    "print(f'Observed fraction={p_hat:.4f}')\n",
    "\n",
    "# draw the decision line\n",
    "decision(lowerCI=lc, upperCI=uc, x=p_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;To get a better idea of what could be going wrong, it's a good idea to look at the day by day data again. One good thing to check is whether any particular day stands out as causing the problem or whether it seems to be an overall pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1['p_hat'] = df_w1['# cookies control'] / (df_w1['# cookies control'] + df_w1['# cookies experiment'])\n",
    "df_w2['p_hat'] = df_w2['# cookies control'] / (df_w2['# cookies control'] + df_w2['# cookies experiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Data for Week 1**\n",
    "> |   Day |   # cookies control |   # cookies experiment | $\\hat p$ |\n",
    "> |------:|--------------------:|-----------------------:|---------:|\n",
    "> |   Mon |                5077 |                   4877 | 0.510172 |\n",
    "> |  Tues |                5495 |                   4729 | <span style=\"background-color: yellow;\">0.537846</span> |\n",
    "> |   Wed |                5294 |                   5063 | 0.511739 |\n",
    "> | Thurs |                5446 |                   5035 | 0.519748 |\n",
    "> |   Fri |                5126 |                   5010 | 0.505426 |\n",
    "> |   Sat |                3382 |                   3193 | 0.514824 |\n",
    "> |   Sun |                2891 |                   3226 | <span style=\"background-color: yellow;\">0.472065</span> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Data for Week 2**\n",
    "> | Day   |   # cookies control |   # cookies experiment | $\\hat p$ |\n",
    "> |:------|--------------------:|-----------------------:|---------:|\n",
    "> | Mon   |                5029 |                   5092 | 0.496888 |\n",
    "> | Tues  |                5166 |                   5048 | 0.505776 |\n",
    "> | Wed   |                4902 |                   4985 | 0.495803 |\n",
    "> | Thurs |                4923 |                   4805 | 0.506065 |\n",
    "> | Fri   |                4816 |                   4741 | 0.503924 |\n",
    "> | Sat   |                3411 |                   2939 | <span style=\"background-color: yellow;\">0.537165</span> |\n",
    "> | Sun   |                3496 |                   3075 | <span style=\"background-color: yellow;\">0.532035</span> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; We can see there are some abnormal probabilities, then, we can：  \n",
    "&emsp;&emsp;&emsp;1. Talk to the engineers.  \n",
    "&emsp;&emsp;&emsp;2. Try slicing to see if one particular slice is weired.  \n",
    "&emsp;&emsp;&emsp;3. check age of cookies: does one group have more new cookies?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"background-color: #90ccc3\">What happens if one of your sanity checks fails?</span>   \n",
    "&emsp;&emsp;  \n",
    "  If your sanity checks fail, do not pass go. <span style=\"background-color: #edd2c4\">Do not proceed</span>, go straight to analyzing why your sanity checks fail. You really have to <span style=\"background-color: #edd2c4\">debug</span> and understand what's going on before you can actually analyze your experiment. Because, if you try and move on past that, your conclusions are almost certainly wrong.  \n",
    "&emsp;&emsp;  \n",
    "- <span style=\"background-color: #90ccc3\">How do you figure out what went wrong?</span>   \n",
    "&emsp;&emsp;  \n",
    "   - First, something might have gone wrong <span style=\"background-color: #edd2c4\">technically</span>, and you want to work with your engineers to understand, is there something going on with the experiment infrastructure? Did I get the experiment set up correctly? Is something wrong with experiment diversion? You really want to debug the experiment setup with the engineers. \n",
    "   - Second, you can try and do a <span style=\"background-color: #edd2c4\">retrospective analysis</span>. Try and recreate experiment diversion from the data capture, and understand that this is something endemic to what you're trying to do that may be causing the situation. \n",
    "   - Third, we can try and use <span style=\"background-color: #edd2c4\">pre and post periods</span> we talked about earlier in lesson four. If you're in a pre-period, then you can say, did I see the same changes in those invariance in my pre-period? If I saw them in the pre-period and the experiment, that points to a problem with the experiment infrastructure, the set up, something along those lines. On the other hand, if you see the change only in your experiment but not in the pre-period, that points to something with the experiment itself, maybe the data capture or something along those lines.   \n",
    "&emsp;&emsp;  \n",
    "- <span style=\"background-color: #90ccc3\">What are the most common reasons you've seen for data not matching up like this?</span>   \n",
    "&emsp;&emsp;  \n",
    "  1. <span style=\"background-color: #edd2c4\">data capture</span>. especially when you want to capture a new experience that the user is undergoing. And so maybe you just didn't capture it correctly. Maybe the change triggers very rarely, and you capture it correctly in the experiment, but you don't capture quickly in the control, and so you're not comparing like with like. That's probably the most common.  \n",
    "  2. <span style=\"background-color: #edd2c4\">the experiment's set up</span>. So, for example, what happens if you have a filter to English only? Maybe you set up for the experiment, but not the control, and now your publishes aren't comparable. More rarely, it could be your infrastructure, or the ex, you know, the experiment system, something along those lines. Maybe there's something going on to really sort of reset cookies, and that's what's sort of screwing things up. You just have to go through and test all of them.  \n",
    "&emsp;&emsp;  \n",
    "- <span style=\"background-color: #90ccc3\">Is it ever just a real difference?</span>   \n",
    "&emsp;&emsp;  \n",
    "  It could be, but it's probably not a good thing if it is.  \n",
    "  For example, there's something in the infrastructure that's going on to cause the cookies to churn. You probably don't want that, your user's probably not having a good experience, and it's not good for the experiment itself. Right? Now the key thing to remember is that all of these comparisons, they're approximations. So you're not going to get an exactly the same number. It's just going to be approximately the same.  \n",
    "  The other thing that I get asked a lot is, well, maybe my users are learning over time, maybe it's, you know, users are adapting to the change. Now, the key thing there is that, if there really is a learning effect, then you're going to see not very much change in the beginning, and it's going to be increasing over time. So if you're seeing a big change right from the beginning, it's probably not a learning effect. Then if all the sanity checks do pass, we can analyze the experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Evaluation with Single or Multiple Metrics  \n",
    "\n",
    "#### **Single Metric**  \n",
    "Our goal is to make a business decision about whether your experiment has favorably impacted your metrics.  \n",
    "It means you want to decide if you've observed a statistically significant result of your experiment. Now, typically we also want to estimate the magnitude and the direction of the change. Then once you have all that information you can make a decision about whether you want to recommend that your business actually launch this experiment.   \n",
    "    \n",
    "**What if our results are not statistically significant?**  \n",
    "- break it down into different platforms, different days of the week. This can not only help you find bugs in your experiment setup, but it might give you a new hypothesis about how people are reacting to the experiment.  \n",
    "- cross checking with other methods. you could use the non parametric sign tests that we talked about earlier. To compare the results to what you got from your parametric hypothesis test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"background-color: #90ccc3\">Experiement 1:</span>   \n",
    "> - Change color and placement of \"start now\" button.  \n",
    "> \n",
    "> Metric: \n",
    "> - CTR   \n",
    "> \n",
    "> Unit of diversion: \n",
    "> - Cookie  \n",
    ">\n",
    "> Some date we have:\n",
    "> - $d_{min}$=0.01  ;  $\\alpha$=0.05  ;  $\\beta$=0.2  \n",
    "> - Empirical SE: 0.0035 with 10000 pageviews per group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   control clicks |   control pageviews |   experiment clicks |   experiment pageviews |\n",
      "|-----------------:|--------------------:|--------------------:|-----------------------:|\n",
      "|               51 |                1292 |                 115 |                   1305 |\n",
      "|               39 |                 853 |                  73 |                    835 |\n",
      "|               64 |                1129 |                  91 |                   1133 |\n",
      "|               43 |                 873 |                  60 |                    871 |\n",
      "|               55 |                1197 |                  78 |                   1134 |\n",
      "|               44 |                1023 |                  72 |                   1015 |\n",
      "|               56 |                1003 |                  76 |                    977 |\n",
      "|              353 |                7370 |                 565 |                   7270 |\n"
     ]
    }
   ],
   "source": [
    "# set up data\n",
    "data = {'Day': [\"Day1\", \"Day2\", \"Day3\", \"Day4\", \"Day5\", \"Day6\", \"Day7\"],\n",
    "        'control clicks': [51, 39,64,43,55,44,56],\n",
    "        'control pageviews': [1292,853,1129,873,1197,1023,1003],\n",
    "        'experiment clicks': [115,73,91,60,78,72,76],\n",
    "        'experiment pageviews': [1305,835,1133,871,1134,1015,977]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "totals_df = pd.DataFrame({'Day': ['Total'], **totals}).reindex(columns=df.columns)\n",
    "# Concatenate the original DataFrame and the totals DataFrame\n",
    "df = pd.concat([df, totals_df], ignore_index=True)\n",
    "df.set_index('Day', inplace=True)\n",
    "mkdtable(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SE ~ $\\sqrt{\\frac{1}{N_1}+\\frac{1}{N_2}}$ , so, we get:  \n",
    "> $\\frac{0.0035}{\\sqrt{\\frac{1}{10000}+\\frac{1}{10000}}}=\\frac{\\text{SE}}{\\sqrt{\\frac{1}{7370}+\\frac{1}{7270}}}$, then, SE=0.0041"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated difference=0.0298\n",
      "Margin of Error=0.0080\n",
      "Confidence Interval=0.0218 to 0.0379\n"
     ]
    }
   ],
   "source": [
    "SE=0.0041\n",
    "N_cont=df.loc['Total','control pageviews']\n",
    "N_exp=df.loc['Total','experiment pageviews']\n",
    "X_cont=df.loc['Total','control clicks']\n",
    "X_exp=df.loc['Total','experiment clicks']\n",
    "d_estimated=X_exp/N_exp-X_cont/N_cont\n",
    "print(f'Estimated difference={d_estimated:.4f}')\n",
    "m=SE*1.96\n",
    "print(f'Margin of Error={m:.4f}')\n",
    "lc=d_estimated-m\n",
    "uc=d_estimated+m\n",
    "print(f'Confidence Interval={lc:.4f} to {uc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAA6CAYAAACUGjTOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADG0lEQVR4nO3dv08TYRzH8c81+KMVDEJQGrtQwuKmi8Y4w8CIM4zGoSQsjgKbI2VxhlnYIOluoi7yD5jGATwSgdRAWgTSc3gCCQxcwV6Pu+/7tTS53pGneXjTp0+BekEQBAJgQibuAQDoHIIHDCF4wBCCBwwheMAQggcMIXjAEIIHDCF4wBCCBwwh+BC+72tubk6+78c9FFyCeWoNwYfwfV/z8/N8I91wzFNrCB4whOABQwgeMITgAUMIHjCE4AFDCB4whOABQ7riHgA6LAik3V3p4EDq7pb6+yXPi3tU6BCe4a2o1aRyWRoZkQYGpKEhdzsy4o7XanGPEB1A8BZUKlKhIM3MSNXq+fuqVXe8UHDnIdUIPu0qFWl8XGo03HL+4scQnB5rNNx5RJ9qBJ9mtZo0MeGCbjYvP7fZdOdNTLC8TzGCT7OlJaleD4/9VLPpzl9ejnZciE0ku/T1o5MovmwsGscnZ7eJelxBoLuLi/IkXWUPPpAUlMs6fPM2Ubv3iZ2nELnb7U00kuCfvE/P68C/2z8kSa8/ftGdwd8xj6Z1D+p/tHFxg64FXhDIq1b18t0n1bL3IxhZNJI6T2F+fhhv69djSZ9S944P/+v67qNGm0aCm8SL4tNj07Sk2tj4rlcvnuvz1296+vRZ3MNp3c6Oco/z1768/mvb/VJOQiR2nkIkYknf7kHGKXur6+w2UY8r/0gaHnbvs1/lZ7rnScWicoMPE/UaPrHz1GEs6dPK86RS6XrXTk8nKna0juDTbGpKyuWkTIvTnMm48ycnox0XYkPwadbbK62suGfrsOgzGXfe6qq7DqlE8Gk3NiatrUnZrAv64lL99Fg2K62vS6Oj8YwTHUHwFoyNSZub0sKCVCyev69YdMe3tojdALYzrejtdZtxpZK0tyft70s9PVJfHxt0hhC8NZ7n3l9P0HvsaB+W9IAhBA8YQvCAIQQPGELwgCEEDxhC8IAhBB8in89rdnZW+fz1/7Yc0WOeWhPJP8AAcDPxDA8YQvCAIQQPGELwgCEEDxhC8IAhBA8YQvCAIQQPGELwgCH/AGVn09eNAoTbAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 300x50 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decision(lowerCI=lc,upperCI=uc,x=d_estimated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The result is statistically significant.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Sign test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   control clicks |   control pageviews |   experiment clicks |   experiment pageviews |   CTR control |   CTR experiment |\n",
      "|-----------------:|--------------------:|--------------------:|-----------------------:|--------------:|-----------------:|\n",
      "|               51 |                1292 |                 115 |                   1305 |     0.0394737 |        0.0881226 |\n",
      "|               39 |                 853 |                  73 |                    835 |     0.045721  |        0.0874251 |\n",
      "|               64 |                1129 |                  91 |                   1133 |     0.0566873 |        0.0803177 |\n",
      "|               43 |                 873 |                  60 |                    871 |     0.0492554 |        0.0688863 |\n",
      "|               55 |                1197 |                  78 |                   1134 |     0.0459482 |        0.0687831 |\n",
      "|               44 |                1023 |                  72 |                   1015 |     0.0430108 |        0.070936  |\n",
      "|               56 |                1003 |                  76 |                    977 |     0.0558325 |        0.0777892 |\n",
      "|              353 |                7370 |                 565 |                   7270 |     0.0478969 |        0.0777166 |\n"
     ]
    }
   ],
   "source": [
    "df['CTR control']=df['control clicks']/df['control pageviews']\n",
    "df['CTR experiment']=df['experiment clicks']/df['experiment pageviews']\n",
    "mkdtable(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Day with higer CTR in experiment compared with control(positive change): 7/7  \n",
    "> \n",
    "> If no difference: 50% chance of positive change on each day(like flipping a fair coin). Here, we just have 7 days, which cannot make a normal distribution assumption. Here, we can use an [online calculator](https://www.graphpad.com/quickcalcs/binomial1.cfm) to get the probability. The two-tail P value is 0.0156. This is the probability of observing a result at lease this extreme by chance. 0.0156<0.05, which means the sign test agrees with the hypothesis test, that this result was unlikely to come about by chance.\n",
    ">\n",
    "> Recommandation: Launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"background-color: #90ccc3\">Experiement 2:</span>  \n",
    "> \n",
    "> Metric: \n",
    "> - CTR  \n",
    ">\n",
    "> Some data we have:\n",
    "> - $d_{min}$=0.01  ;  $\\alpha$=0.05  ;  $\\beta$=0.2  \n",
    "> - Empirical SE: 0.0062 with 5000 pageviews in each group.  \n",
    "> - Control pageviews: 27948 ;  Control CTR: 0.1016  \n",
    "> - Experiment pageviews: 28052;  Experiment CTR: 0.1131   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Sanity Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data\n",
    "N_cont_pageview=27948\n",
    "N_exp_pageview=28052\n",
    "CTR_control=0.1016\n",
    "CTR_experiment=0.1131\n",
    "d_min=0.01\n",
    "alpha=0.05\n",
    "SE_empirical=0.0062"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated difference=0.0115\n",
      "Standard Error=0.0026\n",
      "Margin of Error=0.0051\n",
      "Confidence Interval=0.0064 to 0.0166\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAA6CAYAAACUGjTOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADG0lEQVR4nO3dv08TYRzH8c81+KMVDEJQGrtQwuKmi8Y4w8CIM4zGoSQsjgKbI2VxhlnYIOluoi7yD5jGATwSgdRAWgTSc3gCCQxcwV6Pu+/7tTS53pGneXjTp0+BekEQBAJgQibuAQDoHIIHDCF4wBCCBwwheMAQggcMIXjAEIIHDCF4wBCCBwwh+BC+72tubk6+78c9FFyCeWoNwYfwfV/z8/N8I91wzFNrCB4whOABQwgeMITgAUMIHjCE4AFDCB4whOABQ7riHgA6LAik3V3p4EDq7pb6+yXPi3tU6BCe4a2o1aRyWRoZkQYGpKEhdzsy4o7XanGPEB1A8BZUKlKhIM3MSNXq+fuqVXe8UHDnIdUIPu0qFWl8XGo03HL+4scQnB5rNNx5RJ9qBJ9mtZo0MeGCbjYvP7fZdOdNTLC8TzGCT7OlJaleD4/9VLPpzl9ejnZciE0ku/T1o5MovmwsGscnZ7eJelxBoLuLi/IkXWUPPpAUlMs6fPM2Ubv3iZ2nELnb7U00kuCfvE/P68C/2z8kSa8/ftGdwd8xj6Z1D+p/tHFxg64FXhDIq1b18t0n1bL3IxhZNJI6T2F+fhhv69djSZ9S944P/+v67qNGm0aCm8SL4tNj07Sk2tj4rlcvnuvz1296+vRZ3MNp3c6Oco/z1768/mvb/VJOQiR2nkIkYknf7kHGKXur6+w2UY8r/0gaHnbvs1/lZ7rnScWicoMPE/UaPrHz1GEs6dPK86RS6XrXTk8nKna0juDTbGpKyuWkTIvTnMm48ycnox0XYkPwadbbK62suGfrsOgzGXfe6qq7DqlE8Gk3NiatrUnZrAv64lL99Fg2K62vS6Oj8YwTHUHwFoyNSZub0sKCVCyev69YdMe3tojdALYzrejtdZtxpZK0tyft70s9PVJfHxt0hhC8NZ7n3l9P0HvsaB+W9IAhBA8YQvCAIQQPGELwgCEEDxhC8IAhBB8in89rdnZW+fz1/7Yc0WOeWhPJP8AAcDPxDA8YQvCAIQQPGELwgCEEDxhC8IAhBA8YQvCAIQQPGELwgCH/AGVn09eNAoTbAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 300x50 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_estimated=CTR_experiment-CTR_control\n",
    "SE=0.0062/math.sqrt(1/5000+1/5000)*math.sqrt(1/N_cont_pageview+1/N_exp_pageview)\n",
    "print(f'Estimated difference={d_estimated:.4f}')\n",
    "print(f'Standard Error={SE:.4f}')\n",
    "m=SE*1.96\n",
    "print(f'Margin of Error={m:.4f}')\n",
    "lc=d_estimated-m\n",
    "uc=d_estimated+m\n",
    "print(f'Confidence Interval={lc:.4f} to {uc:.4f}')\n",
    "decision(lowerCI=lc,upperCI=uc,x=d_estimated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So, the result is statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Sign test**   \n",
    "> <img src=\"image/17.png\" width=\"800\">    \n",
    ">   \n",
    "> Days where CTR is higher in experiment compared with control: 9/14. From the online calculator, the two-tail P value is 0.4240, which is > 0.05. The result of sign test shows it is not statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you do when the sign test and the hypothesis test don't agree?  \n",
    "- consider Simpson's Paradox: there's a bunch of different subgroups in your data like user populations. And within each subgroup, the results are stable but when you aggregate them all together, it's the mix of subgroups that actually drives your result.  \n",
    "- Simpson's Paradox: a real example about graduate admissions at Berkeley. \n",
    "    <img src=\"image/18.png\" width=\"600\">\n",
    "   - In aggregate, if you looked at the number of people who are accepted divided by the number that applied, the rate of women being accepted (46%) was statistically significantly lower than men being accepted (52%), which seemed bad. But when you looked at it by department, there were actually departments where women were accepted at a higher rate than men. So how can that be? The answer turned out to be that you had to look at it by department, because the acceptance rates by department were variable. And what was happening is that more women were applying to the smaller departments that had a very low acceptance rate. So when you aggregated the numbers, ignoring department, you saw women accepted at a lower rate. But if you looked at each department individually, the rates were very comparable between men and women. \n",
    "   - In experiment, this happening on our experiment because you have these subgroups like people who use it more on weekdays or people use it more on weekends. And you may find that, for example, new users are correlated with weekend use and experienced users who react differently are correlated with weekday use. And so what sometimes you find in these cases is that what drives the results of your experiment are how many people from each group you get. Within each group, their behavior is stable, you can get a statistically significant result or an insignificant result. But when you add them all together, all the changes in your traffic mix are driving the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Multiple Metrics**  \n",
    "\n",
    "When you run evaluations of multiple metrics at the same time, the more things you test, the more likely you are to see significant differences just by chance. So if you're testing 20 metrics, and you have a 95% confidence level. You would expect to see one case at least that time where you got a result that says it's significant but it's only concurring by chance. So this is a problem, but you're not sunk because it shouldn't be repeatable. That is if you did the same experiment on another day or you divide or just slices or you did some bootstrap analysis, you wouldn't see the same metric showing up as significant differences every time, it should occur randomly. There's another technique for this called [multiple comparisons](https://en.wikipedia.org/wiki/Multiple_comparisons_problem) that adjusts your significance level, so that it accounts for how many metrics or how many different tests you're doing. One thing a lot of people like to do an experiment frameworks is do automatic detection of differences. So if you're doing exploratory data analysis you can reanalyze your data and make sure the same metric isn't popping up every time and see if the differences are repeatable. But if you want to set up say automatic alerting that tells you, one of my metrics was significantly different on this experiment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Experiment:  \n",
    "> - Prompt students to contact coach more frequently.  \n",
    "> \n",
    "> Metrics:  \n",
    "> - Probability that student signs up for coaching  \n",
    "> - How early students sign up for coaching  \n",
    "> - Average price paid per student  \n",
    ">   \n",
    "> Assume $\\alpha$=0.05, for 3 metrics, the chance of at least 1 false positive:  \n",
    "> - P(FP >= 1)= 1- P( FP = 0 ) = 1- 0.95 * 0.95 * 0.95=0.143 (assuming independent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem: Probability of any false positive increases as you increase number of metrics.**   \n",
    "Solution: Use higher confidence level for each metric.  \n",
    "- Method 1: assume independence:  \n",
    "&emsp;&emsp;\n",
    "  - $\\alpha_{overall}=1-(1-\\alpha_{individual})^n$   \n",
    "  - This assumption allows us to apply the Bonferroni correction.  \n",
    "&emsp;&emsp;\n",
    "- Method 2: [Bonferroni correction](https://en.wikipedia.org/wiki/Bonferroni_correction)  \n",
    "&emsp;&emsp;\n",
    "  - The Bonferroni correction is a method used to control the familywise error rate (FWER) in hypothesis testing when conducting <span style=\"background-color: #edd2c4\">multiple statistical comparisons simultaneously</span>. In the context of A/B testing, you might consider using the Bonferroni correction when you are running multiple comparisons, such as testing multiple variants or looking at several different metrics simultaneously. <span style=\"background-color: #edd2c4\">The goal is to reduce the likelihood of making Type I errors (false positives) when conducting multiple hypothesis tests.</span>  \n",
    "  - FWER represents the probability of making at least one Type I error across all these tests. This is the overarching error rate that we want to control when conducting multiple hypothesis tests simultaneously.\n",
    "  - How Bonferroni correction works:  \n",
    "    - $\\alpha_{individual}=\\frac{\\alpha_{overall}}{N}$. N stands for the number of pairwise comparisons. \n",
    "    - $\\alpha_{individual}$ is also known as Bonferroni-corrected significance level. This is the significance level we used in each comparison group.  \n",
    "    - Perform individual hypothesis tests, compare the p-value obtained with $\\alpha_{individual}$.\n",
    "&emsp;&emsp;\n",
    "  - Advantage of this method:  \n",
    "    - Simple\n",
    "    - no assumption\n",
    "    - conservative: guaranteed to give $\\alpha_{overall}$ at least as small as specified.  \n",
    "&emsp;&emsp;\n",
    "- Method 3: Control FDR  \n",
    "&emsp;&emsp;\n",
    "  - Control False Discovery Rate ( FDR = $E[\\frac{ \\text{\\# false positive} }{\\text{\\# rejections}}]$ ): suppose you have 200 metrics, cap FDR at 0.05. This means you're okay with 5 false positives and 95 true positives in every experiment.  \n",
    "  - Compared to Bonferroni correction, this method is less conservative. It is always more suitable when conducting a large number of hypothesis tests when controlling FWER is not neccessary.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When we settle on a bunch of metrics, how to make recommendations?**  \n",
    "\n",
    "-  There are multiple comparisons or things along lines when you have multiple metrics. We hope that related metrics are all going to move in the same direction. <span style=\"font-size: smaller;\"><span style=\"color: grey\">For example, if you're measuring both click-through rate and click-through probability, hopefully they'll move in the same direction.</span></span> Another thing might happen when you have a composite metric is that multiple metrics can be unruly.<span style=\"font-size: smaller;\"><span style=\"color: grey\"> For example, RPM which is revenue per thousand queries is composed of both click through rate, as well as cost per clicks and so if you see. RPM of them one direction. Hopefully you can look at click through rate in cost per click to understand why it moved in that direction. Let's say you've decided that reading time on the page or stay time is a good signal. People like your page. But clicks are also a good signal. And so then you might see that when you make a UI change to the page people spend less time reading, but more time clicking. And then you really have to understand how people are reacting to the change because you can't quantitatively evaluate which one is better.</span></span> That's why sometimes people want to single overall evaluation criteria, so they can make the decision based on that. The main question when you try and come up with an OEC(overall evaluation criteria), is how do you find a good one? You really need to understand what your company is doing and what the problems are in order to try and come up with balancing. <span style=\"font-size: smaller;\"><span style=\"color: grey\">For example, balance state time and clicks. Is it seventy percent and thirty percent. Is it twenty five and seventy-five. Who knows or maybe. Quite frankly. Clicks is a bad measure for user experience.</span></span> An OEC doesn't absolve you of understanding why metrics are moving in these different directions. What it can be helpful with this balancing long term investment <span style=\"font-size: smaller;\"><span style=\"color: grey\">(like return visits to the site)</span></span> with short term day to day metrics <span style=\"font-size: smaller;\"><span style=\"color: grey\">(like increased clicks)</span></span>.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you come up with one?**\n",
    "\n",
    "- Typically, there's a lot of validation in the process, but it usually begins with some form of business analysis. For instance, our company as a whole may aim for a 25% increase in revenue along with a 75% boost in site usage. Typically, we start from this point. Once we have a few candidates, we want to conduct a whole bunch of different experimentsto validate their impact and whether they steer us in the right direction. The challenge is to avoid overly planning based on the company's expectations for these experiments, which might obscure unexpected changes. At Google, we took a somewhat different approach. We gathered all the decision makers who had made launch decisions about hundreds of experiments. We presented them with experiment results without disclosing the specific test or the actual launch decision. We asked them to re-evaluate whether they would launch the changes based solely on the experiment results. . From this, we re-calibrated the weights for an OEC. We came up with an OEC that way, but whenever we tried using it to make a launch decision, everyone would look at the OEC to see what were the individual metrics that actually changed. So we never actually ended up using the OEC to actually make a decision. Now what the sort of tells us is that having an OEC doesn't have to be a formal number. It's really just trying to encapsulate what your company cares about. And how much you're going to be balancing.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once we've identified metrics with significant changes, what's the next step?**  \n",
    "- We need to determine what our results do and don't tell us. Statistically significant results indicate an impact on the user experience, but we must understand the nature of the change and decide if we should proceed with the launch. What if we have statistically significant changes in some metrics but not in others? Again, understanding the change is crucial. Intuition and experience from previous experiments can help. For instance, minor changes in one metric but no change in others might be acceptable, but for significant changes, it could indicate a problem.  \n",
    "- The same principle applies to different user segments or slices. If a change has a positive impact on one slice but not on another, we must investigate why. It could be related to user preferences, prior observations in similar experiments, or even a technical issue. <span style=\"font-size: smaller;\"><span style=\"color: grey\">Is it a question about having different users, and how much they like or don't like the change? Have you seen that effect in other experiments? Do you have a bug? These are all possibilities when you have changes in one slice versus another. Let me give you an example. Bolding, right, we like to basically bold certain words in order to give them emphasis. Now, bolding in like English or German, or those types of alphabets, works really well for providing emphasis. But if you try and bold in Japanese, or Korean, or Chinese, a bolded character is actually really hard to read. And it actually makes it harder for a user to read the word than if it was not bolded. And so in that situation, you may actually want to want something different for that slice. Maybe use a different color as opposed to doing a bold. And then, bottom line, how do you decide whether to launch your change or not?</span></span> Ultimately, deciding whether to launch a change requires asking several questions. Are the results both statistically and practically significant? Do we understand the impact on the user experience? Is it worth implementing the change? The key is to provide a recommendation based on judgment, not just the technical aspects <span style=\"font-size: smaller;\"><span style=\"color: grey\">(designing the experiment and running it and sizing it properly, and having all of the metric chosen correctly)</span></span> of the experiment. What matters is whether the recommendation to launch or not was the right one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is it ever advisable to gradually increase the exposure of your experiment?** <span style=\"font-size: smaller;\"><span style=\"color: grey\">For instance, you might begin by diverting only 1% of your traffic to the experiment group and then incrementally raise this proportion until the feature is fully launched.</span></span> \n",
    "\n",
    "- it's a prudent practice to implement a ramp-up when preparing to launch a change. This approach is consistently applied to all our launches at Google. Additionally, we remove all filters during this phase. Suppose you initially tested your change only on English-speaking users. In that case, during the ramp-up, it's essential to extend the testing to all users. This broader approach helps us identify any unintended impacts on unaffected users that may not have been part of the original experiment. However, there's an interesting challenge to consider during the ramp-up: the effects may flatten out as the change is gradually introduced. \n",
    "\n",
    "**What about cases where the initial effect was statistically significant? Isn't the point of statistical significance to ensure repeatability of results?** \n",
    "\n",
    "- Several straightforward reasons can explain why effects may not be repeatable. Seasonality effects, <span style=\"font-size: smaller;\"><span style=\"color: grey\">such as those related to student behavior during summer vacation or holidays like Black Friday and Cyber Monday, can significantly alter online behavior.</span></span> To account for these seasonal or event-driven impacts, we use a method called a \"holdback.\" This involves launching the change to everyone except for a small group of users who don't receive the change, allowing us to compare their behavior to the control group. Over time, this analysis helps us determine if the results are indeed repeatable and capture seasonal or event-driven effects.\n",
    "\n",
    "**Are there other factors that can cause this vanishing launch effect?**  \n",
    "- Indeed, we've touched on this topic in previous lessons. Factors like a novelty effect or change aversion can influence user behavior. As users discover, adopt, or modify their adoption of your change, their behavior and the measured effects can shift. Cohort analysis can be especially useful in such cases. Additionally, in situations involving advertisers with budgets, improper budget controls can lead to changing effects as you ramp up.\n",
    "\n",
    "**How can I detect these issues during the experiment rather than discovering them after the launch has occurred and something has gone awry?** \n",
    "- To address these complexities, you'll need to delve into more advanced analyses and experiment designs. As discussed in lesson four, employing pre- and post-periods can be beneficial. If you're concerned about learning effects, combining pre- and post-periods with a cohort analysis can help you understand how users adapt to the change over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. **Check.** Double check, triple check that your experiment was set up properly. Look at your end variance. Check that your experiment metrics are actually looking sane. you have to check that before you try and do anything else. \n",
    "\n",
    "2. **Significant.** Remember you aren't just looking for statistical significance. You're really making a business decision. So what if you have an experiment that statistically significantly improves the experience for 30% of users but it's neutral for everyone else. Do you necessarily want to launch it as it is or do you want to try to make it better. And what if you're in a situation where it improves things for 70% but makes it worse for 30%. So you actually need to make a decision about whether you want to launch the feature as it is, or if you want to try to fine tune it first. \n",
    "\n",
    "3. **Overall business analysis.** For example, what's the engineering cost of maintaining the change? Are there customer support or sales issues? Overall what's the opportunity cost if you actually choose to launch the change relative to the rewards you're going to get from the change or potentially not launching the change? This is all about the judgment call with regards to the user experience and the business. And ultimately that's what your commission has to be based on. \n",
    "\n",
    "If you haven't actually tested your change on all traffic, even if it only impacts a small slice, now's a good time to test for that incidental impact. If this is the first experiment you run, especially if it's a really big or important feature, you might want to actually try running a couple of different experiments. Analyzing the results and then seeing how you feel about the results of your first experiment before you decide to launch it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
